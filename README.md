1. Wolf, Michael E., and Monica S. Lam. "[A loop transformation theory and an algorithm to maximize parallelism](https://homes.luddy.indiana.edu/achauhan/Teaching/B629/2006-Fall/CourseMaterial/1991-tpds-wolf-unimodular.pdf)." IEEE Transactions on Parallel & Distributed Systems 2.04 (1991): 452-471.
1. Lamport, Leslie. "[The parallel execution of DO loops](https://www.microsoft.com/en-us/research/publication/2016/12/The-Parallel-Execution-of-DO-Loops.pdf)." Communications of the ACM 17.2 (1974): 83-93.
1. [Tiramisu: A Polyhedral Compiler for Expressing Fast and Portable Code](https://arxiv.org/abs/1804.10694) https://github.com/Tiramisu-Compiler/tiramisu
1. [Persistent RNNs](https://svail.github.io/persistent_rnns/)
1. [Optimizing RNNs with Differentiable Graphs](http://svail.github.io/diff_graphs/)
1. [Optimizing RNN performance](http://svail.github.io/rnn_perf/)
1. [Optimizing Recurrent Neural Networks in cuDNN 5](https://developer.nvidia.com/blog/optimizing-recurrent-neural-networks-cudnn-5/)
1. [Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training](https://ieeexplore.ieee.org/abstract/document/9138914)
1. Appleyard J, Kocisky T, Blunsom P. [Optimizing performance of recurrent neural networks on gpus](https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=optimizing+RNN+performance+on+GPU&btnG=)[J]. arXiv preprint arXiv:1604.01946, 2016.